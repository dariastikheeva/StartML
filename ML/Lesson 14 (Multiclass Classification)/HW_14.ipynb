{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "378da103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ef23b3",
   "metadata": {},
   "source": [
    "Мы будем работать с данными агрегатора такси [Sigma Cabs](https://www.kaggle.com/datasets/arashnic/taxi-pricing-with-mobility-analytics). В зависимости от характеристик поездки требуется предсказать один из трех типов повышенного ценообразования: [1, 2, 3]. Таким образом, это поможет компании оптимально мэтчить такси и клиентов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1aec896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131662, 14)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('sigma_cabs.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bf69f8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trip_Distance</th>\n",
       "      <th>Type_of_Cab</th>\n",
       "      <th>Customer_Since_Months</th>\n",
       "      <th>Life_Style_Index</th>\n",
       "      <th>Confidence_Life_Style_Index</th>\n",
       "      <th>Destination_Type</th>\n",
       "      <th>Customer_Rating</th>\n",
       "      <th>Cancellation_Last_1Month</th>\n",
       "      <th>Var1</th>\n",
       "      <th>Var2</th>\n",
       "      <th>Var3</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Surge_Pricing_Type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>T0005689460</th>\n",
       "      <td>6.77</td>\n",
       "      <td>B</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.42769</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>3.90500</td>\n",
       "      <td>0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>46</td>\n",
       "      <td>60</td>\n",
       "      <td>Female</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T0005689461</th>\n",
       "      <td>29.47</td>\n",
       "      <td>B</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.78245</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>3.45000</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>56</td>\n",
       "      <td>78</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T0005689464</th>\n",
       "      <td>41.58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E</td>\n",
       "      <td>3.50125</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56</td>\n",
       "      <td>77</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T0005689465</th>\n",
       "      <td>61.56</td>\n",
       "      <td>C</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>3.45375</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52</td>\n",
       "      <td>74</td>\n",
       "      <td>Male</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T0005689467</th>\n",
       "      <td>54.95</td>\n",
       "      <td>C</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.03453</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>3.40250</td>\n",
       "      <td>4</td>\n",
       "      <td>51.0</td>\n",
       "      <td>49</td>\n",
       "      <td>102</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Trip_Distance Type_of_Cab  Customer_Since_Months  \\\n",
       "Trip_ID                                                         \n",
       "T0005689460           6.77           B                    1.0   \n",
       "T0005689461          29.47           B                   10.0   \n",
       "T0005689464          41.58         NaN                   10.0   \n",
       "T0005689465          61.56           C                   10.0   \n",
       "T0005689467          54.95           C                   10.0   \n",
       "\n",
       "             Life_Style_Index Confidence_Life_Style_Index Destination_Type  \\\n",
       "Trip_ID                                                                      \n",
       "T0005689460           2.42769                           A                A   \n",
       "T0005689461           2.78245                           B                A   \n",
       "T0005689464               NaN                         NaN                E   \n",
       "T0005689465               NaN                         NaN                A   \n",
       "T0005689467           3.03453                           B                A   \n",
       "\n",
       "             Customer_Rating  Cancellation_Last_1Month  Var1  Var2  Var3  \\\n",
       "Trip_ID                                                                    \n",
       "T0005689460          3.90500                         0  40.0    46    60   \n",
       "T0005689461          3.45000                         0  38.0    56    78   \n",
       "T0005689464          3.50125                         2   NaN    56    77   \n",
       "T0005689465          3.45375                         0   NaN    52    74   \n",
       "T0005689467          3.40250                         4  51.0    49   102   \n",
       "\n",
       "             Gender  Surge_Pricing_Type  \n",
       "Trip_ID                                  \n",
       "T0005689460  Female                   2  \n",
       "T0005689461    Male                   2  \n",
       "T0005689464    Male                   2  \n",
       "T0005689465    Male                   3  \n",
       "T0005689467    Male                   2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Занесем индекс колонку\n",
    "df = df.set_index('Trip_ID')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e242cb4",
   "metadata": {},
   "source": [
    "Описание признаков:\n",
    "\n",
    "1. **Trip_ID**: ID for TRIP\n",
    "2. **Trip_Distance**: The distance for the trip requested by the customer\n",
    "3. **TypeofCab**: Category of the cab requested by the customer\n",
    "4. **CustomerSinceMonths**: Customer using cab services since n months; 0 month means current month\n",
    "5. **LifeStyleIndex**: Proprietary index created by Sigma Cabs showing lifestyle of the customer based on their behaviour\n",
    "6. **ConfidenceLifeStyle_Index**: Category showing confidence on the index mentioned above\n",
    "7. **Destination_Type**: Sigma Cabs divides any destination in one of the 14 categories.\n",
    "8. **Customer_Rating**: Average of life time ratings of the customer till date\n",
    "9. **CancellationLast1Month**: Number of trips cancelled by the customer in last 1 month\n",
    "10. **Var1**, **Var2** and **Var3**: Continuous variables masked by the company. Can be used for modelling purposes\n",
    "11. **Gender**: Gender of the customer\n",
    "\n",
    "**SurgePricingType**: Target (can be of 3 types)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dcb1b9",
   "metadata": {},
   "source": [
    "### EDA \n",
    "Заполните пропуски в вещественных признаках медианой, а в категориальных - самым популярным классом. Изобразите марицу корреляций и выведите топ5 пар самых коррелированных признаков.\n",
    "\n",
    "Так как в сумме уникальных значений различных категориальных признаков окажется не супер-много, примените `One-Hot-Encoding` для них. Не забудьте в методе `pd.get_dummies` указать параметр `drop_first=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "408b1b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code is here\n",
    "num_cols = []\n",
    "str_cols = []\n",
    "for col in list(df.columns):\n",
    "    if df[col].dtype == object:\n",
    "        str_cols.append(col)\n",
    "    else:\n",
    "        num_cols.append(col)\n",
    "\n",
    "for col in num_cols:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "for col in str_cols:\n",
    "    \n",
    "    df[col] = df[col].fillna( df.groupby(col,as_index=False)[col].value_counts().sort_values(by='count', ascending=False)[col].iloc[0] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "070eb630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Absolute Correlations\n",
      "Var2                      Var3                  0.683437\n",
      "Trip_Distance             Life_Style_Index      0.468332\n",
      "Life_Style_Index          Var3                  0.303324\n",
      "Customer_Rating           Var2                  0.302968\n",
      "Trip_Distance             Var3                  0.231706\n",
      "Customer_Rating           Var3                  0.227531\n",
      "Life_Style_Index          Var2                  0.215944\n",
      "Trip_Distance             Var2                  0.200456\n",
      "Life_Style_Index          Customer_Rating       0.189165\n",
      "Cancellation_Last_1Month  Surge_Pricing_Type    0.185646\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "### Секретные функции для фильтрации признаков\n",
    "\n",
    "def get_redundant_pairs(df):\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "\n",
    "def get_top_abs_correlations(df, n=5):\n",
    "    au_corr = df.corr().abs().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    return au_corr[0:n]\n",
    "\n",
    "print(\"Top Absolute Correlations\")\n",
    "print(get_top_abs_correlations(df[num_cols], 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bcf14cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "for col in str_cols:\n",
    "    \n",
    "    ### Your code is here\n",
    "    df = df.join(pd.get_dummies(df[col], prefix=col, drop_first=True))\n",
    "    df = df.drop(columns=[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff21e11",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "635d7536",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2022)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bc1906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Surge_Pricing_Type'])\n",
    "y = df['Surge_Pricing_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6f4f4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, \n",
    "                                                     test_size=0.2, \n",
    "                                                     shuffle=True, \n",
    "                                                     random_state=2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e04467",
   "metadata": {},
   "source": [
    "**Задание 1.** Обучите One-vs-Rest Logreg. Не забудьте в шаг добавить стандартизацию данных (через `StandardScaler`) Посчитайте precision, recall, f1-score и усредните по всем классам с помощью micro, macro и weighted avg. Здесь и далее округляйте до 3 знака после запятой.\n",
    "\n",
    "Чтобы отдельно и долго не вычислять метрики, можно воспользоваться `classification_report` из `sklearn.metrics`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48524b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2      0.636     0.834     0.722     11349\n",
      "           3      0.741     0.571     0.645      9612\n",
      "           1      0.723     0.542     0.619      5372\n",
      "\n",
      "    accuracy                          0.679     26333\n",
      "   macro avg      0.700     0.649     0.662     26333\n",
      "weighted avg      0.692     0.679     0.673     26333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "### Your code is here\n",
    "pipe = Pipeline([('scaler', StandardScaler()),('one_vs_all', OneVsRestClassifier(estimator = LogisticRegression()))])\n",
    "pipe.fit(X_train, y_train)\n",
    "pred = pipe.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred, labels=list(y.unique()), digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d085b5fa",
   "metadata": {},
   "source": [
    "Подберите оптимальные гиперпараметры модели с помощью `GridSearchCV()` из предложенных. Для лучшего набора гиперпараметров посчитайте те же самые метрики. Валидировать параметры необходимо по `accuracy`. В этот раз проведем настояющую процедуру Кросс-Валидации! \n",
    "\n",
    "Для этого в метод `fit` передадим тренировочную часть наших данных, в параметр `cv` ничего не будем передавать (по дефолту 5-fold Кросс-Валидация будет проведена), а итоговые метрики замерим на тесте!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74d481ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'one_vs_all__estimator__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "              'one_vs_all__estimator__C': [0.001, 0.01, 0.1, 1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9d9c6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2      0.635     0.839     0.723     11349\n",
      "           3      0.742     0.576     0.649      9612\n",
      "           1      0.742     0.534     0.621      5372\n",
      "\n",
      "    accuracy                          0.681     26333\n",
      "   macro avg      0.706     0.650     0.664     26333\n",
      "weighted avg      0.696     0.681     0.675     26333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Your code is here\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "search = GridSearchCV(pipe, param_grid)\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "pred_grid = search.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred_grid, labels=list(y.unique()), digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcd8543",
   "metadata": {},
   "source": [
    "Изобразите три калибровочные кривые для Logistic Classifier: 0-vs-rest, 1-vs-rest, 2-vs-rest. Хорошо ли откалиброван обученный классификатор? \n",
    "\n",
    "Заметьте, что `predict_proba` возвращает список из вероятностей для всех наших классов!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4196f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code is here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ae8e9a",
   "metadata": {},
   "source": [
    "**Задание 2.** Обучите логистическую регрессию с гиперпараметрами из первого задания на полиномиальных признаках до 4 степени. Сравните метрики с первым заданием.\n",
    "\n",
    "\n",
    "Пример: Пусть у нас был единственный признак \n",
    "\n",
    "$$\n",
    "d_j = [1, 2, 3, 4]\n",
    "$$\n",
    "\n",
    "Тогда полиномиальные признаки до 4 степени от такого будут иметь вид:\n",
    "\n",
    "$$\n",
    "d_j^1 = [1, 2, 3, 4]\n",
    "$$\n",
    "\n",
    "$$\n",
    "d_j^2 = [1, 4, 9, 16]\n",
    "$$\n",
    "\n",
    "$$\n",
    "d_j^3 = [1, 8, 27, 64]\n",
    "$$\n",
    "\n",
    "$$\n",
    "d_j^4 = [1, 16, 81, 256]\n",
    "$$\n",
    "\n",
    "P.S. Бинарные колонки нет смысла возводить в какие-то степени, поэтому возьмем исключительно вещественные из базовых. \n",
    "\n",
    "Для этого можно воспользоваться классическим циклом (или уроком из занятия про `Sberbank Housing Market`). Положите модифицированный датасет в переменную `X_polinomial`!\n",
    "\n",
    "P.S.S Зачастую еще, создаваю полиномиальные фичи, учитывают \"пересечения\" признаков, то есть, например, из векторов признаков $d_j, d_i$ генерируют не просто новые степени $d_j^2, d_i^2, d_j^3, d_i^3...$, а еще и признаки вида $d_j \\cdot d_i, d_j^2 \\cdot d_i, d_j \\cdot d_i^2...$, но здесь ограничьтесь просто степенями!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "460c8e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Создание полиномиальных признаков\n",
    "\n",
    "X_polinomial = X.copy()\n",
    "\n",
    "\n",
    "### Your code is here\n",
    "for col in num_cols[:-1]:\n",
    "    for power in [2, 3, 4]:\n",
    "        \n",
    "        to_add = (X_polinomial[col]**power).to_frame().rename({col:f\"{col}_{power}\"}, axis=1)\n",
    "        X_polinomial = pd.concat((X_polinomial, to_add), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4311cdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pol_train, X_pol_test, y_train, y_test  = train_test_split(X_polinomial, y, \n",
    "                                                             test_size=0.2, \n",
    "                                                             shuffle=True, \n",
    "                                                             random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f53437b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2      0.636     0.837     0.723     11349\n",
      "           3      0.741     0.584     0.653      9612\n",
      "           1      0.748     0.532     0.622      5372\n",
      "\n",
      "    accuracy                          0.682     26333\n",
      "   macro avg      0.708     0.651     0.666     26333\n",
      "weighted avg      0.697     0.682     0.677     26333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Your code is here\n",
    "pipe_pol = Pipeline([('scaler', StandardScaler()),('one_vs_all', OneVsRestClassifier(estimator = LogisticRegression(C=0.001, penalty='l2')))])\n",
    "pipe_pol.fit(X_pol_train, y_train)\n",
    "pred_pol = pipe_pol.predict(X_pol_test)\n",
    "print(classification_report(y_test, pred_pol, labels=list(y.unique()), digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c0029b",
   "metadata": {},
   "source": [
    "По аналогии с первым заданием изобразите три калибровочные кривые. Стало ли лучше?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fce01e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code is here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f69fc1d",
   "metadata": {},
   "source": [
    "**Задание 3.** Обучите на датасете без полиномиальных признаков One-vs-One `SGDClassifier` из `sklearn.linear_model`, который использует стохастический градиентный спуск (узнаете о нем позже) и может обучать как `SVM`, так и, например, `LogReg`, если указать в качестве параметра `loss` либо `hinge`, либо `log` соответственно!\n",
    "\n",
    "Посчитайте precision, recall, f1-score и усредните по всем классам с помощью micro, macro и weighted avg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4fcc722",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, \n",
    "                                                     test_size=0.2, \n",
    "                                                     shuffle=True, \n",
    "                                                     random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e23ec85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2      0.625     0.876     0.730     11349\n",
      "           3      0.763     0.529     0.625      9612\n",
      "           1      0.745     0.520     0.613      5372\n",
      "\n",
      "    accuracy                          0.677     26333\n",
      "   macro avg      0.711     0.642     0.656     26333\n",
      "weighted avg      0.700     0.677     0.668     26333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "### Your code is here\n",
    "pipe = Pipeline([('scaler', StandardScaler()),('one_vs_one', OneVsOneClassifier(estimator = SGDClassifier()))])\n",
    "pipe.fit(X_train, y_train)\n",
    "pred = pipe.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred, labels=list(y.unique()), digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30228d24",
   "metadata": {},
   "source": [
    "Подберите оптимальные гиперпараметры модели с помощью `GridSearchCV()`. При этом переберите всевозможные функции потерь. Таким образом, при `loss = 'hinge'`, мы обучим SVM, при `loss = 'log'` мы обучим логистическую регрессию и т.д.\n",
    "\n",
    "Используйте прием с Кросс-Валидацией при подборе параметров, как ранее, а также замерьте метрики на тесте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0cae4cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'one_vs_one__estimator__loss': ['hinge', 'log', 'modified_huber'],\n",
    "              'one_vs_one__estimator__penalty': ['l1', 'l2'],\n",
    "              'one_vs_one__estimator__alpha': [0.001, 0.01, 0.1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7cdebdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2      0.625     0.876     0.730     11349\n",
      "           3      0.762     0.535     0.628      9612\n",
      "           1      0.756     0.517     0.615      5372\n",
      "\n",
      "    accuracy                          0.679     26333\n",
      "   macro avg      0.715     0.643     0.658     26333\n",
      "weighted avg      0.702     0.679     0.669     26333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Your code is here\n",
    "search = GridSearchCV(pipe, param_grid)\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "pred_grid = search.predict(X_test)\n",
    "print(classification_report(y_test, pred_grid, labels=list(y.unique()), digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afac649",
   "metadata": {},
   "source": [
    "Можно ли однозначно сказать, какой подход оказался лучше: One-vs-Rest или One-vs-One?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
